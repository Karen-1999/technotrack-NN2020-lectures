# Обучение сетей прямого распространения.
### Часть 1: Метод вычисления градиента функции потерь. Оптимизация функции потерь.


Дополнительные материалы для самостоятельного изучения:

Большая и очень интересная статья об импульсном методе градиентной оптимизации с интерактивными визуализациями: [link](https://distill.pub/2017/momentum/)
<br />
Сайт, посвященный визуализации ландшафта функций потерь: много красивых визуализаций: [link](https://losslandscape.com)
<br />
Статья с подробным описанием метода визуализации ландшафта функции многомерной потерь в нашем трехмерном пространстве: [link](https://towardsdatascience.com/loss-landscapes-and-the-blessing-of-dimensionality-46685e28e6a4)
<br />
Статья на arxiv.org от группы [Дмитрия Ветрова](https://www.hse.ru/staff/dvetrov) о ландшафте функции потерь и связных областях: [link](https://arxiv.org/abs/1802.10026)
<br />
Большая обзорная статья о методах оптимизации нейронных сетей: [link](https://arxiv.org/abs/1912.08957)
<br />
Также рекомендуем обратить внимание на курс [Евгения Голикова](https://github.com/varenick) и [Ивана Скороходова](https://github.com/universome) по теоретическим основам Deep Learning, где подробно разбирают в том числе статьи Kenji Kawaguchi, которые мы упоминали: [link](https://github.com/deepmipt/tdl). Там же есть лекция, посвященная information bottleneck, теории, объясняющей обучение сетей с точки зрения теории информации. Это не обязательно к ознакомлению, но очень интересно. По этой же теме есть более популярное изложение: [link](https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/).